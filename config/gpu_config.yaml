quant:
    a_qconfig_low: # layer quant config for low bitwidth quantizer
        quantizer: LSQFakeQuantize
        observer: MSEObserver
        bit: 4
        symmetric: False
        ch_axis: -1
    a_qconfig_med: # layer quant config for medium bitwidth quantizer
        quantizer: LSQFakeQuantize
        observer: MSEObserver
        bit: 5
        symmetric: False
        ch_axis: -1
    a_qconfig_high: # layer quant config for high bitwidth quantizer
        quantizer: LSQFakeQuantize
        observer: MSEObserver
        bit: 6
        symmetric: False
        ch_axis: -1
                
    w_qconfig:
        quantizer: AdaRoundFakeQuantize
        observer: MSEObserver
        bit: 8
        symmetric: False
        ch_axis: 0

    w_qconfig_low:
        quantizer: AdaRoundFakeQuantize
        observer: MSEObserver
        bit: 4
        symmetric: False
        ch_axis: 0
    w_qconfig_med:
        quantizer: AdaRoundFakeQuantize
        observer: MSEObserver
        bit: 5
        symmetric: False
        ch_axis: 0
    w_qconfig_high:
        quantizer: AdaRoundFakeQuantize
        observer: MSEObserver
        bit: 6
        symmetric: False
        ch_axis: 0
    calibrate: 1024
    recon:
        batch_size: 32
        scale_lr: 4.0e-5
        warm_up: 0.2
        weight: 0.01
        iters: 10000 #20000
        b_range: [20, 2]
        keep_gpu: True
        round_mode: learned_hard_sigmoid
        drop_prob: 0.5
        mixed_p: 0.5
    ptmq:
        lambda1: 0.4
        lambda2: 0.3
        lambda3: 0.3
        mixed_p: 0.5
        gamma1: 100
        gamma2: 100
        gamma3: 100
model:                    # architecture details
    type: mobilenetv2        # model name
    kwargs:
        num_classes: 1000
    path: "/content/ptmq_log_after/mobilenetv2.pth"
data:
    path: "/content/ptmq_log_after/imagenet"
    batch_size: 256
    num_workers: 16
    pin_memory: True
    input_size: 224
    test_resize: 256
process:
    seed: 1005
